import pytest
from llm.tokenizer import LocalTokenizer, VertexTokenizer, GeminiAPITokenizer
from config import GOOGLE_API_KEY

TEST_TEXTS = [
    "ì•ˆë…•í•˜ì„¸ìš”. This is a mixed sentence with English and í•œê¸€.",
    "ğŸš€ ì—ì–´ë“œë ì°¸ì—¬ ê°€ì´ë“œ:\n1. ì±„ë„ ê°€ì…\n2. í¼ ì‘ì„±\n3. ì¸ì¦ ì™„ë£Œ\n\n#ì—ì–´ë“œë #í† í°",
    "ê¸´ ë¬¸ë‹¨ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ì˜ˆì‹œ:\n```python\ndef hello():\n    print('hi')\n```\nì—¬ê¸°ê¹Œì§€.",
    "1. Visit https://dropspy.com\n2. Follow @DropSpy\n3. Claim now!\n\nğŸ”¥ Hurry up!",
]


@pytest.mark.parametrize("text", TEST_TEXTS)
def test_tokenizers(text):
    local = LocalTokenizer()
    print(f"LocalTokenizer: {local.count_tokens(text)} tokens")
    try:
        vertex = VertexTokenizer(model_name="gemini-1.5-flash-001")
        print(f"VertexTokenizer: {vertex.count_tokens(text)} tokens")
    except Exception as e:
        print(f"VertexTokenizer error: {e}")
    try:
        api_key = GOOGLE_API_KEY
        if api_key:
            gemini = GeminiAPITokenizer(api_key=api_key, model_name="gemini-2.0-flash")
            print(f"GeminiAPITokenizer: {gemini.count_tokens(text)} tokens")
    except Exception as e:
        print(f"GeminiAPITokenizer error: {e}")
