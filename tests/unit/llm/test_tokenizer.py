import pytest
from llm.tokenizer import LocalTokenizer, VertexTokenizer

TEST_TEXTS = [
    "ì•ˆë…•í•˜ì„¸ìš”. This is a mixed sentence with English and í•œê¸€.",
    "ğŸš€ ì—ì–´ë“œë ì°¸ì—¬ ê°€ì´ë“œ:\n1. ì±„ë„ ê°€ì…\n2. í¼ ì‘ì„±\n3. ì¸ì¦ ì™„ë£Œ\n\n#ì—ì–´ë“œë #í† í°",
    "ê¸´ ë¬¸ë‹¨ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ ì˜ˆì‹œ:\n```python\ndef hello():\n    print('hi')\n```\nì—¬ê¸°ê¹Œì§€.",
    "1. Visit https://dropspy.com\n2. Follow @DropSpy\n3. Claim now!\n\nğŸ”¥ Hurry up!",
]

expected_local_tokens = [33, 53, 54, 38]
expected_vertex_tokens = [15, 39, 34, 27]


@pytest.mark.parametrize(
    "text, expected_local, expected_vertex",
    zip(TEST_TEXTS, expected_local_tokens, expected_vertex_tokens),
)
def test_tokenizers(text, expected_local, expected_vertex):
    local = LocalTokenizer()
    local_tokens = local.count_tokens(text)
    assert local_tokens == expected_local
    try:
        vertex = VertexTokenizer(model_name="gemini-1.5-flash-001")
        vertex_tokens = vertex.count_tokens(text)
        assert vertex_tokens == expected_vertex
    except AssertionError as e:
        raise AssertionError(f"VertexTokenizer tokens mismatch: {e}")
    except Exception as e:
        print(f"VertexTokenizer error: {e}")
