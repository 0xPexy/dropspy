# .env.example
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments and will be ignored.

# --- Telegram API Credentials & Settings ---
# Obtain these from my.telegram.org
TELEGRAM_API_ID=12345678
TELEGRAM_API_HASH="your_telegram_api_hash_here"
# Name for the Telethon session file (e.g., my_dropspy_session).
# This file will be created in your project's root directory or where specified by Telethon.
TELEGRAM_SESSION_NAME="my_telegram_session"

# Comma-separated list of target Telegram chat usernames (e.g., @channelname),
# public group aliases, or numeric chat/channel IDs.
# Example: TELEGRAM_TARGET_CHATS="@my_channel_username,regular_username,-1001234567890,anothergroup"
TELEGRAM_TARGET_CHATS="some_chat_username_or_id,another_chat_username_or_id"

# Default number of past days to fetch messages from for new chats (if no prior fetch record exists).
TELEGRAM_DEFAULT_FETCH_DAYS=7

# --- Google Generative AI API Settings ---
# Your Google AI Studio API Key (https://makersuite.google.com/app/apikey)
GOOGLE_API_KEY="your_google_ai_api_key_here"

# --- Model Configurations for DropSpy ---
# The Gemini model name to be used for token counting and 1st/2nd pass filtering (lightweight tasks).
# Ensure this model is available via the API and suits your needs for speed and cost.
# Example: "gemini-1.5-flash-latest", "gemini-2.0-flash-lite"
LIGHTWEIGHT_MODEL_NAME="gemini-2.0-flash-lite"

# The total context window size (in tokens) of the LIGHTWEIGHT_MODEL_NAME.
# Refer to official Google documentation for the specific model you choose.
# e.g., 1048576 for Gemini 1.5 Flash (often referred to as 1M).
# For Gemini 2.0 Flash-Lite, as per user-provided table, it also supports a large context.
LIGHTWEIGHT_MODEL_CONTEXT_WINDOW_SIZE=1000000

# Target percentage of the lightweight model's context window to utilize for each API call in filtering passes (e.g., 0.80 for 80%).
# This provides a safety margin for tokenization variations and model overhead.
BATCH_TARGET_WINDOW_PERCENTAGE=0.80

# (Optional) Model name for the final, high-performance analysis pass
# Example: "gemini-1.5-pro-latest", "gemini-2.5-pro-preview" (if using)
# FINAL_ANALYSIS_MODEL_NAME="gemini-1.5-pro-latest"

# --- File Paths and Directories ---
# The root directory where all application-generated data (batches, logs, records) will be stored.
DATA_DIRECTORY_ROOT="data"

# Path to the base prompt markdown file used for the 1st pass LLM filtering.
# This path is typically relative to your project's root directory.
FIRST_PASS_PROMPT_FILE_PATH="prompts/1st_pass_filter_prompt.md"

# (Optional) Path to the prompt file for the 2nd pass LLM filtering (news/Reddit context processing).
# SECOND_PASS_PROMPT_FILE_PATH="prompts/2nd_pass_context_filter_prompt.md"

# (Optional) Path to the prompt file for the final analysis 
# FINAL_ANALYSIS_PROMPT_FILE_PATH="prompts/final_analysis_prompt.md"

# --- Logging Configuration (Optional) ---
# Set the logging level. Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"